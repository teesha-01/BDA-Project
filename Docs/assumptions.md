# Assumptions

1. **Input Data Consistency**:  
   The input data structure remains consistent throughout the pipeline.

2. **Fraud Labels**:  
   Fraud detection models rely on labeled data from the dataset for training.

3. **Technology Stack**:  
   Kafka, Spark, HBase, and Streamlit are chosen based on scalability needs and are assumed to be appropriate for the pipeline's requirements.
